{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '/home/heza7322/PycharmProjects/missing-value-handling-in-carts/example_notebooks/data_raw/'\n",
    "output_folder_path = '/home/heza7322/PycharmProjects/missing-value-handling-in-carts/example_notebooks/data_cleaned/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# German credit score\n",
    "data_set = 'german.data'\n",
    "raw_data = pd.read_fwf(folder_path + data_set, header = None)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['checking_account_status'] = raw_data[0].str[1:].astype(int).replace({14:np.nan})\n",
    "df['checking_account_status'] -= df['checking_account_status'].min()\n",
    "\n",
    "more_raw_data = raw_data[1].str.split(' ',expand = True)\n",
    "more_raw_data.columns = np.arange(2,more_raw_data.shape[1]+2)\n",
    "\n",
    "df['duration'] = more_raw_data.pop(2).astype(float)\n",
    "df['credit_history'] = more_raw_data.pop(3)\n",
    "df['purpose'] = more_raw_data.pop(4).replace({\n",
    "    'A40': 'car (new)',\n",
    "    'A41': 'car (used)',\n",
    "    'A42': 'furniture/equipment',\n",
    "    'A43': 'radio/TV',\n",
    "    'A44': 'domestic_appliances',\n",
    "    'A45': 'repairs',\n",
    "    'A46': 'education',\n",
    "    'A47': 'vacation',\n",
    "    'A48': 'retraining',\n",
    "    'A49': 'business',\n",
    "    'A410': 'others'\n",
    "})\n",
    "\n",
    "df['credit_amount'] = more_raw_data.pop(5).astype(float)\n",
    "df['savings'] = more_raw_data.pop(6).str[1:].astype(float).replace({65:np.nan})\n",
    "df['savings'] -= df['savings'].min()\n",
    "df['savings'].value_counts(dropna=False)\n",
    "\n",
    "df['present_employment_since'] = more_raw_data.pop(7).str[1:].astype(float)\n",
    "df['present_employment_since'] -= df['present_employment_since'].min()\n",
    "\n",
    "df['installment_rate'] = more_raw_data.pop(8).astype(float)\n",
    "\n",
    "df['gender'] = more_raw_data[9].replace({\n",
    "    'A91':'male',\n",
    "    'A92': 'female',\n",
    "    'A93': 'male',\n",
    "    'A94': 'male',\n",
    "    'A95': 'female'\n",
    "})\n",
    "df['marital_status'] = more_raw_data[9].replace({\n",
    "    'A91': 'divorced/separated/married',\n",
    "    'A92': 'divorced/separated/married',\n",
    "    'A93': 'single',\n",
    "    'A94': 'divorced/separated/married',\n",
    "    'A95': 'single'\n",
    "})\n",
    "more_raw_data.drop(9,axis=1,inplace=True)\n",
    "\n",
    "df['other_debtors'] = more_raw_data.pop(10).replace({\n",
    "    'A101': 'none',\n",
    "    'A102': 'co-applicant',\n",
    "    'A103': 'guarantor'\n",
    "})\n",
    "\n",
    "df['residence_since'] = more_raw_data.pop(11).astype(float)\n",
    "df['property'] = more_raw_data.pop(12).replace({\n",
    "    'A121': 'real_estate',\n",
    "    'A122': 'building',\n",
    "    'A123': 'car or other',\n",
    "    'A124': 'unknown/none'\n",
    "})\n",
    "df['age'] = more_raw_data.pop(13).astype(float)\n",
    "\n",
    "df['other_installment_plans'] = more_raw_data.pop(14).replace({\n",
    "    'A141': 'bank',\n",
    "    'A142': 'stores',\n",
    "    'A143': 'none'\n",
    "})\n",
    "\n",
    "df['housing'] = more_raw_data.pop(15).replace({\n",
    "    'A151': 'rent',\n",
    "    'A152': 'own',\n",
    "    'A153': 'free'\n",
    "})\n",
    "\n",
    "df['n_credit_cards'] = more_raw_data.pop(16).astype(float)\n",
    "df['job'] = more_raw_data.pop(17).str[1:].astype(float)\n",
    "df['job'] -= df['job'].min()\n",
    "\n",
    "df['n_people_liable'] = more_raw_data.pop(18).astype(float)\n",
    "\n",
    "df['telephone'] = more_raw_data.pop(19).replace({\n",
    "    'A191': 'none',\n",
    "    'A192': 'yes'\n",
    "})\n",
    "\n",
    "df['foreign'] = more_raw_data.pop(20).replace({'A201':'yes','A202':'no'})\n",
    "\n",
    "df['y'] = more_raw_data[21].replace({'1': 'bad', '2':'good'})\n",
    "\n",
    "df.dropna().to_csv(output_folder_path+'german_credit.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# ionospehere\n",
    "df = pd.read_fwf(folder_path + 'ionosphere.data', header = None)[0].str.split(',',expand=True)\n",
    "df.drop(1,inplace=True,axis=1)\n",
    "df.rename(columns = {df.columns[-1]:'y'},inplace=True)\n",
    "df['y'].replace({'g':'good','b':'bad'},inplace = True)\n",
    "df.dropna().to_csv(output_folder_path + 'ionosphere_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "# kr-vs-kp\n",
    "df = pd.read_fwf(folder_path + 'kr-vs-kp.data',header = None)[0].str.split(',',expand=True)\n",
    "df['y'] = df.pop(df.columns[-1])\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'kr_vs_kp_cleaned.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "# sonar\n",
    "df = pd.read_fwf(folder_path + 'sonar.all-data',header = None)[0].str.split(',',expand=True)\n",
    "df['y'] = df.pop(df.columns[-1]).replace({'R':'rock','M':'mine'})\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'sonar_cleaned.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "# balance scale\n",
    "df = pd.read_fwf(folder_path + 'balance-scale.data',header = None)[0].str.split(',',expand=True)\n",
    "df.columns = ['y','left_weight','left_distance','right_weight','right_distance']\n",
    "for feature in df.columns[1:]:\n",
    "    df[feature] = df[feature].astype(float)\n",
    "\n",
    "df['y'].replace({\n",
    "    'B':'balance',\n",
    "    'L':'left',\n",
    "    'R':'right'\n",
    "}, inplace = True)\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'balance_scale_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "# iris\n",
    "df = pd.read_fwf(folder_path + 'iris.data',header = None)[0].str.split(',',expand=True)\n",
    "df.columns = ['sepal_length','sepal_width','petal_length','petal_width','y']\n",
    "for feature in df.columns[:-1]:\n",
    "    df[feature] = df[feature].astype(float)\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'iris_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "# lymphography\n",
    "raw_data = pd.read_fwf(folder_path + 'lymphography.data',header = None)[0].str.split(',',expand=True)\n",
    "raw_data.columns = np.arange(1,len(raw_data.columns)+1)\n",
    "df = pd.DataFrame()\n",
    "df['y'] = raw_data[1].replace({\n",
    "    '1':'normal',\n",
    "    '2':'metastases',\n",
    "    '3': 'malign',\n",
    "    '4': 'fibrosis'\n",
    "})\n",
    "\n",
    "df['lymphatics'] = raw_data[2].replace({\n",
    "    '1':'normal',\n",
    "    '2':'arched',\n",
    "    '3':'deformed',\n",
    "    '4':'displaced'\n",
    "})\n",
    "\n",
    "df['block_of_affere'] = raw_data[3].replace({\n",
    "    '1':'no',\n",
    "    '2':'yes'\n",
    "})\n",
    "df['block_of_lymph_c'] = raw_data[4].replace({\n",
    "    '1': 'no',\n",
    "    '2': 'yes'\n",
    "})\n",
    "df['block_of_lymph_s'] = raw_data[5].replace({\n",
    "    '1': 'no',\n",
    "    '2': 'yes'\n",
    "})\n",
    "df['by_pass'] = raw_data[6].replace({\n",
    "    '1': 'no',\n",
    "    '2': 'yes'\n",
    "})\n",
    "df['extravasates'] = raw_data[7].replace({\n",
    "    '1': 'no',\n",
    "    '2': 'yes'\n",
    "})\n",
    "df['regeneration'] = raw_data[8].replace({\n",
    "    '1': 'no',\n",
    "    '2': 'yes'\n",
    "})\n",
    "df['early_uptake'] = raw_data[9].replace({\n",
    "    '1': 'no',\n",
    "    '2': 'yes'\n",
    "})\n",
    "df['lymph_nodes_dimin'] = raw_data[10].astype(float)\n",
    "df['lymph_nodes_enlar'] = raw_data[11].astype(float)\n",
    "df['chamges_in_lymph'] = raw_data[12].replace({\n",
    "    '1':'bean',\n",
    "    '2':'oval',\n",
    "    '3':'round'\n",
    "})\n",
    "\n",
    "df['defect_in_node'] = raw_data[13].replace({str(i+1):cat for i,cat in enumerate(['no', 'lacunar', 'lac. marginal', 'lac. central'])})\n",
    "df['change_in_node'] = raw_data[14].replace({str(i+1):cat for i,cat in enumerate(['no', 'lacunar', 'lac. marginal', 'lac. central'])})\n",
    "df['changes_in_stru'] = raw_data[15].replace({str(i+1):cat for i,cat in enumerate(['no', 'grainy', 'drop-like', 'coarse', 'diluted', 'reticular', 'stripped', 'faint'])})\n",
    "df['special_forms'] = raw_data[16].replace({str(i+1):cat for i,cat in enumerate(['no', 'chalices','vesicles'])})\n",
    "df['dislocation'] = raw_data[17].replace({str(i+1):cat for i,cat in enumerate(['no', 'yes'])})\n",
    "df['exclusion'] = raw_data[18].replace({str(i+1):cat for i,cat in enumerate(['no', 'yes'])})\n",
    "df['n_nodes'] = 4.5* (raw_data[19].astype(float)-1)*10\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'lymphography_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [],
   "source": [
    "# glass\n",
    "df = pd.read_fwf(folder_path + 'glass.data',header = None)[0].str.split(',',expand=True)\n",
    "df.drop(0,axis=1,inplace=True)\n",
    "df.columns = ['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','y']\n",
    "df['y'].replace({'':'4'},inplace=True)\n",
    "\n",
    "for feature in df.columns[:-1]:\n",
    "    df[feature] = df[feature].astype(float)\n",
    "\n",
    "df['y'].replace({\n",
    "   '1': 'building_windows_float_processed',\n",
    "   '2': 'building_windows_non_float_processed',\n",
    "   '3': 'vehicle_windows_float_processed',\n",
    "   '4': 'vehicle_windows_non_float_processed',\n",
    "   '5': 'containers',\n",
    "   '6': 'tableware',\n",
    "   '7': 'headlamps',\n",
    "},inplace=True)\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'glass.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "df_train = pd.read_fwf(folder_path + 'sat.trn',header = None)[0].str.split(' ',expand=True)\n",
    "df_test = pd.read_fwf(folder_path + 'sat.tst',header = None)[0].str.split(' ',expand=True)\n",
    "df = pd.concat([df_train,df_test])\n",
    "\n",
    "df['y'] = df.pop(df.columns[-1]).replace({\n",
    "'1': 'red_soil',\n",
    "'2': 'cotton_crop',\n",
    "'3': 'grey_soil',\n",
    "'4': 'damp_grey_soil',\n",
    "'5': 'soil_with_vegetation_stubble',\n",
    "'6': 'mixture_class',\n",
    "'7': 'very_damp_grey_soil'\n",
    "})\n",
    "\n",
    "df[df.columns[:-1]] = df[df.columns[:-1]].astype(float)\n",
    "df.dropna().to_csv(output_folder_path + 'satimage_cleaned.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [
    "# Image segmentation\n",
    "df_train = pd.read_fwf(folder_path + 'segmentation.data',header=None).loc[2:][0].str.split(',',expand=True)\n",
    "df_train.columns = ['y'] + list(df_train.loc[2])[:-1]\n",
    "df_train.drop(2,axis=0,inplace=True)\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test = pd.read_fwf(folder_path + 'segmentation.test',header=None).loc[2:][0].str.split(',',expand=True)\n",
    "df_test.columns = ['y'] + list(df_test.loc[2])[:-1]\n",
    "df_test.drop(2,axis=0,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df = pd.concat([df_train,df_test])\n",
    "df[df.columns[1:]] =df[df.columns[1:]].astype(float)\n",
    "df['y'] = df['y'].str.lower()\n",
    "df.dropna().to_csv(output_folder_path + 'segmentation_cleaned.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "# zoo\n",
    "df = pd.read_fwf(folder_path + 'zoo.data',header=None)[0].str.split(',',expand=True)\n",
    "class_dict = df.groupby(df.columns[-1])[0].unique().apply(lambda l: ', '.join(l)).to_dict()\n",
    "df[df.columns[-1]].replace(class_dict,inplace=True)\n",
    "df.drop(0,axis=1,inplace=True)\n",
    "df.columns = ['hair','feathers','eggs','milk','airborne','aquatic','predator','toothed','backbone','breathes','venomous','fins','legs','tail','domestic','catsize','y']\n",
    "\n",
    "\n",
    "df['legs'] = df['legs'].astype(float)\n",
    "df[['hair','feathers','eggs','milk','airborne','aquatic','predator','toothed','backbone','breathes','venomous','fins','tail','domestic','catsize']]=df[['hair','feathers','eggs','milk','airborne','aquatic','predator','toothed','backbone','breathes','venomous','fins','tail','domestic','catsize']].replace({'0':'false','1':'true'})\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'zoo_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [],
   "source": [
    "# letter recognition\n",
    "df = pd.read_fwf(folder_path + 'letter-recognition.data',header=None)[0].str.split(',',expand=True)\n",
    "y = df.pop(0)\n",
    "df.replace({'':np.nan},inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "for feature in df.columns:\n",
    "    df[feature] = df[feature].astype(float)\n",
    "\n",
    "df['y'] = y\n",
    "\n",
    "df.dropna().to_csv(output_folder_path + 'letter_cleaned.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# loan prediction\n",
    "df = pd.read_csv(folder_path + 'loan_train.csv')\n",
    "df.drop('Loan_ID',axis=1,inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['Dependents'] = df['Dependents'].replace({'3+':'3'}).astype(float)\n",
    "df.dtypes\n",
    "\n",
    "df['y'] = df.pop('Loan_Status')\n",
    "\n",
    "df.to_csv(output_folder_path + 'loan_cleaned.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Boston housing\n",
    "df = pd.read_csv(folder_path + 'BostonHousing.csv')\n",
    "\n",
    "df['y'] = df.pop('medv')\n",
    "\n",
    "df.to_csv(output_folder_path + 'boston_housing.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "# Black Friday\n",
    "df = pd.read_csv(folder_path + 'black_friday.csv')\n",
    "\n",
    "df.drop(['User_ID','Product_ID','Product_Category_1','Product_Category_2','Product_Category_3'],axis=1,inplace=True)\n",
    "\n",
    "df['Age'] = df['Age'].replace({age_range: age_group for age_group,age_range in enumerate(sorted(df['Age'].unique()))})\n",
    "\n",
    "import string\n",
    "df['Occupation'] = df['Occupation'].replace({number:letter for number,letter in enumerate(string.ascii_lowercase)})\n",
    "\n",
    "df['Stay_In_Current_City_Years'] = df['Stay_In_Current_City_Years'].replace({range: group for group,range in enumerate(sorted(df['Stay_In_Current_City_Years'].unique()))})\n",
    "\n",
    "df['y'] = df.pop('Purchase')\n",
    "\n",
    "df.to_csv(output_folder_path + 'black_friday.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# Adult census\n",
    "df = pd.read_csv(folder_path + 'adult.csv')\n",
    "df.columns = [column.replace('.','_') for column in df.columns]\n",
    "df = df.replace({'?':None}).dropna()\n",
    "df.drop(['relationship','education'],axis=1,inplace=True)\n",
    "df['y'] = df.pop('income').replace({'>50K':'low','<=50K':'high'})\n",
    "\n",
    "df.to_csv(output_folder_path + 'income_census.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# titanic\n",
    "df = pd.read_csv(folder_path + 'titanic.csv')\n",
    "df['y'] = df.pop('Survived').replace({0:'no',1: 'yes'})\n",
    "df['n_family'] = df['SibSp'] + df['Parch']\n",
    "features = ['Pclass','Sex','Age','n_family','Fare','Embarked']\n",
    "df = df[features + ['y']].dropna()\n",
    "\n",
    "df.to_csv(output_folder_path + 'titanic.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "# life expectancy\n",
    "df = pd.read_csv(folder_path + 'Life Expectancy Data.csv')\n",
    "df = df.loc[df['Year']==2015]\n",
    "df.drop(['Year','Country','Alcohol','Hepatitis B','Total expenditure'],axis=1,inplace=True)\n",
    "df['y'] = df.pop('Life expectancy ')\n",
    "df.dropna(inplace=True)\n",
    "df.to_csv(output_folder_path + 'life_expectancy.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "# health insurance\n",
    "df = pd.read_csv(folder_path + 'insurance.csv')\n",
    "\n",
    "df['y'] = df.pop('charges')\n",
    "\n",
    "df.to_csv(output_folder_path + 'health_insurance.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "# concrete\n",
    "df = pd.read_csv(folder_path + 'Concrete_Data.csv')\n",
    "\n",
    "df['y'] = df.pop('strength')\n",
    "\n",
    "df.to_csv(output_folder_path + 'cement.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "# online data sharing\n",
    "df = pd.read_csv(folder_path + 'OnlineNewsPopularity.csv')\n",
    "df.columns = [column[1:] for column in df.columns]\n",
    "\n",
    "data_channel_cols = [column for column in df.columns if column[:len('data_channel_is')]=='data_channel_is']\n",
    "df['data_channel'] = df[data_channel_cols].idxmax(axis=1).str[len('data_channel_is_'):]\n",
    "df.drop(data_channel_cols, axis=1, inplace=True)\n",
    "\n",
    "weekday_cols = [column for column in df.columns if column[:len('weekday_is_')]=='weekday_is_']\n",
    "df['weekday'] = df[weekday_cols].idxmax(axis=1).str[len('weekday_is_'):]\n",
    "df.drop(weekday_cols,axis=1,inplace=True)\n",
    "df.drop('is_weekend',axis=1,inplace=True)\n",
    "\n",
    "df['y'] = df.pop('shares')\n",
    "df.head()\n",
    "\n",
    "df.to_csv(output_folder_path + 'online_news.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = pd.read_fwf(folder_path + 'auto-mpg.data', header = None)\n",
    "df.columns = ['mpg','cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name']\n",
    "df.replace({'?':None},inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df['horsepower'] = df['horsepower'].astype(float)\n",
    "\n",
    "df['y'] = df.pop('mpg')\n",
    "\n",
    "df.drop('car_name',axis=1,inplace=True)\n",
    "df.to_csv(output_folder_path + 'auto_mpg.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df_red = pd.read_csv(folder_path + 'winequality-red.csv', sep = ';').assign(color='red')\n",
    "df_white = pd.read_csv(folder_path + 'winequality-white.csv', sep = ';').assign(color='white')\n",
    "\n",
    "df = pd.concat([df_red,df_white])\n",
    "\n",
    "df['y'] = df.pop('quality').astype(float)\n",
    "\n",
    "df.to_csv(output_folder_path + 'wine_quality.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
